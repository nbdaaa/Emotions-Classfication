{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Needed Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "CqdCyBPJAWkD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk, subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split the dataset into Train, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/Processed dataset/processed_data.csv')\n",
    "train_df = pd.read_csv(\"Dataset/Processed dataset/train_data.csv\")\n",
    "test_df = pd.read_csv(\"Dataset/Processed dataset/test_data.csv\")\n",
    "validation_df = pd.read_csv(\"Dataset/Processed dataset/validation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 161613\n",
      "Validation set size: 53871\n",
      "Test set size: 53871\n"
     ]
    }
   ],
   "source": [
    "# Display split sizes\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(validation_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel submissive ever</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feel playful enough try new combination</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>find broken piece feeling nothing feeling noth...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feel ecstatic worry make love automatic adica ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ive feeling really jealous friend rafia im ash...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161608</th>\n",
       "      <td>feeling nervous</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161609</th>\n",
       "      <td>feel like punished believing austin</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161610</th>\n",
       "      <td>look back little paragraph ive written feel bi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161611</th>\n",
       "      <td>feel inconvenienced trimmer blade dull</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161612</th>\n",
       "      <td>feel rather surprised claimed swedish prosecut...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161613 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence sentiment  label\n",
       "0                                    feel submissive ever   sadness      4\n",
       "1                 feel playful enough try new combination       joy      2\n",
       "2       find broken piece feeling nothing feeling noth...     anger      0\n",
       "3       feel ecstatic worry make love automatic adica ...       joy      2\n",
       "4       ive feeling really jealous friend rafia im ash...     anger      0\n",
       "...                                                   ...       ...    ...\n",
       "161608                                    feeling nervous      fear      1\n",
       "161609                feel like punished believing austin   sadness      4\n",
       "161610  look back little paragraph ive written feel bi...     anger      0\n",
       "161611             feel inconvenienced trimmer blade dull   sadness      4\n",
       "161612  feel rather surprised claimed swedish prosecut...  surprise      5\n",
       "\n",
       "[161613 rows x 3 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feeling lake popular weekend summer huge parki...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>couldnt stop feeling threatened card grandmoth...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feel way try ignored ignored got interested es...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling bitchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>know little feel special bond</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53866</th>\n",
       "      <td>think bottom line b story pierce feel need acc...</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53867</th>\n",
       "      <td>straight man ejacalute like week sexual intere...</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53868</th>\n",
       "      <td>feel really rude stating fact would feel rude ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53869</th>\n",
       "      <td>im feeling especially triggered grumpy note ev...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53870</th>\n",
       "      <td>feel useless home</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53871 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment  label\n",
       "0      feeling lake popular weekend summer huge parki...       joy      2\n",
       "1      couldnt stop feeling threatened card grandmoth...      fear      1\n",
       "2      feel way try ignored ignored got interested es...   sadness      4\n",
       "3                                         feeling bitchy     anger      0\n",
       "4                          know little feel special bond       joy      2\n",
       "...                                                  ...       ...    ...\n",
       "53866  think bottom line b story pierce feel need acc...      love      3\n",
       "53867  straight man ejacalute like week sexual intere...      love      3\n",
       "53868  feel really rude stating fact would feel rude ...     anger      0\n",
       "53869  im feeling especially triggered grumpy note ev...     anger      0\n",
       "53870                                  feel useless home   sadness      4\n",
       "\n",
       "[53871 rows x 3 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 im feeling cold im alone\n",
       "1        feel like im th grade shy wouldnt say anything...\n",
       "2                           feel like navy dress dangerous\n",
       "3                               feel jaded chaser although\n",
       "4                  feel petty vicious mean defensive angry\n",
       "                               ...                        \n",
       "53866    feeling especially tender tendency get weepy h...\n",
       "53867    pryers feel like listening perhaps punished ba...\n",
       "53868    promise never react even grievously provoked l...\n",
       "53869                              feel submissive spoiled\n",
       "53870    go period feeling like one could love unremark...\n",
       "Name: sentence, Length: 53871, dtype: object"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    36000\n",
       "0    34341\n",
       "4    33000\n",
       "1    28598\n",
       "3    20699\n",
       "5     8975\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 'sadness', 2: 'joy', 0: 'anger', 3: 'love', 5: 'surprise', 1: 'fear'}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = dict(zip(train_df['label'], train_df['sentiment']))\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "Lt6pqhFIAYSX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, device):\n",
    "        # Make sure all text values are strings\n",
    "        self.text = dataframe['sentence'].astype(str).values\n",
    "        self.labels = dataframe['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Convert 'nan' to empty string if necessary\n",
    "        if text == 'nan':\n",
    "            text = ''\n",
    "        encoding = self.tokenizer.encode(text)\n",
    "        input_ids = torch.tensor(encoding, dtype=torch.long, device=self.device)\n",
    "        label = torch.tensor(label, dtype=torch.long, device=self.device)\n",
    "        return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "7LOszNGrAZRy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    max_length = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "    # All tensors should already be on the correct device from the dataset\n",
    "    device = input_ids[0].device\n",
    "    \n",
    "    input_ids = torch.stack([\n",
    "        torch.cat([ids, torch.zeros(max_length - len(ids), dtype=torch.long, device=device)]) \n",
    "        for ids in input_ids\n",
    "    ])\n",
    "    labels = torch.stack(labels)\n",
    "    return input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "g1rriEeAAcKm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Linear layers for Q, K, V matrices\n",
    "        self.wq = nn.Linear(d_model, d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Output linear transformation\n",
    "        self.dense = nn.Linear(d_model, d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)   self.\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)  # (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
    "        )\n",
    "\n",
    "        # Layer normalization and dropout\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Split the last dimension into (num_heads, depth)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, num_heads, depth)\n",
    "        # Transpose the result to shape (batch_size, num_heads, seq_len, depth)\n",
    "        return x.transpose(1, 2)  # (batch_size, seq_len, num_heads, depth) -> (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        dk = torch.tensor(k.size(-1), dtype=torch.float32)  # scalar\n",
    "        scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        output = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights  # (batch_size, num_heads, seq_len_q, depth_v), (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Apply linear layers and split into heads\n",
    "        q = self.split_heads(self.wq(x), batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "        k = self.split_heads(self.wk(x), batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "        v = self.split_heads(self.wv(x), batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # Apply the custom scaled dot-product attention\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(q, k, v, mask)  # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "\n",
    "        # Transpose and reshape back to (batch_size, seq_len, d_model)\n",
    "        scaled_attention = scaled_attention.transpose(1, 2).contiguous()  # (batch_size, seq_len, num_heads, depth)\n",
    "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Apply the final linear layer to combine the heads\n",
    "        attn_output = self.dense(concat_attention)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layernorm2(x + self.dropout(ff_output))  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        return x  # (batch_size, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3QlTYsoylh3"
   },
   "source": [
    "![](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/2dc6f3e0-5fb4-11ef-9b72-9db6eacc12d1-Screen_Shot_2024_08_21_at_18.54.35.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "id": "rtNUAcArIUmZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a long enough 'positional' tensor and fill it with positional encodings\n",
    "        # [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # (d_model/2,)\n",
    "\n",
    "        # Khởi tạo một tensor `pe` với các giá trị bằng 0, sẽ chứa các giá trị mã hóa vị trí (positional encodings).\n",
    "        # `pe` có dạng (max_len, d_model), trong đó `max_len` là chiều dài tối đa của chuỗi\n",
    "        # và `d_model` là chiều kích thước của embeddings của mô hình.\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "\n",
    "        # Đối với tất cả các vị trí trong chuỗi (từ 0 đến max_len-1) và đối với tất cả các chỉ số chẵn trong các chiều kích thước embedding,\n",
    "        # tính toán giá trị sine của tích `position` và `div_term` và gán nó vào các vị trí tương ứng trong `pe`.\n",
    "        # Gán các giá trị sine cho mọi chiều kích thước khác của embedding (bắt đầu từ chỉ số 0).\n",
    "        # `position` có dạng (max_len, 1) và `div_term` có dạng (d_model/2,).\n",
    "        # Phép toán `position * div_term` được broadcast thành dạng (max_len, d_model/2).\n",
    "        # Kết quả được lưu trữ trong các chỉ số chẵn của `pe`, vì vậy phần được gán có dạng (max_len, d_model/2).\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # (max_len, d_model/2)\n",
    "\n",
    "        # Tương tự, đối với tất cả các vị trí trong chuỗi và đối với tất cả các chỉ số lẻ trong các chiều kích thước embedding,\n",
    "        # tính toán giá trị cosine của tích `position` và `div_term` và gán nó vào các vị trí tương ứng trong `pe`.\n",
    "        # Gán các giá trị cosine cho các chiều kích thước còn lại của embedding (bắt đầu từ chỉ số 1).\n",
    "        # Giống như trước, `position * div_term` được broadcast thành dạng (max_len, d_model/2).\n",
    "        # Kết quả được lưu trữ trong các chỉ số lẻ của `pe`, vì vậy phần được gán có dạng (max_len, d_model/2).\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # (max_len, d_model/2)\n",
    "\n",
    "\n",
    "        # Add a batch dimension and register it as a buffer\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)  # Register as buffer so it's not a parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]  # Add positional encoding, (batch_size, seq_len, d_model)\n",
    "        return x  # (batch_size, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rotary Embedding and apply_rotary_pos_emb function\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super(RotaryEmbedding, self).__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        # Generate a range for sequence length and reshape for broadcasting\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq).unsqueeze(1)\n",
    "        # Calculate the frequency embeddings using broadcasting instead of einsum\n",
    "        freqs = t * self.inv_freq.unsqueeze(0)  # Shape: [seq_len, dim//2]\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)  # Duplicate to match input dimension\n",
    "        return emb[None, :, :]  # Shape: [1, seq_len, dim]\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, sinusoidal_pos):\n",
    "    # Split the query and key tensors into even and odd dimensions\n",
    "    q_cos, q_sin = q[..., 0::2], q[..., 1::2]\n",
    "    k_cos, k_sin = k[..., 0::2], k[..., 1::2]\n",
    "\n",
    "    # Split the positional encodings into cosine and sine parts\n",
    "    cos, sin = sinusoidal_pos[..., 0::2], sinusoidal_pos[..., 1::2]\n",
    "\n",
    "    # Apply rotary embeddings without einsum, element-wise operations\n",
    "    q_rot = torch.cat([q_cos * cos - q_sin * sin, q_cos * sin + q_sin * cos], dim=-1)\n",
    "    k_rot = torch.cat([k_cos * cos - k_sin * sin, k_cos * sin + k_sin * cos], dim=-1)\n",
    "\n",
    "    return q_rot, k_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformer Encoder Layer with Rotary Position Embedding\n",
    "class TransformerEncoderLayerWithROPE(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayerWithROPE, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(self.depth)\n",
    "\n",
    "        # Linear layers for Q, K, V matrices\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output linear transformation\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        # Layer normalization and dropout\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "        dk = torch.tensor(k.size(-1), dtype=torch.float32, device=q.device)\n",
    "        scaled_attention_logits = matmul_qk / torch.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Apply linear layers and split into heads\n",
    "        q = self.split_heads(self.wq(x), batch_size)\n",
    "        k = self.split_heads(self.wk(x), batch_size)\n",
    "        v = self.split_heads(self.wv(x), batch_size)\n",
    "\n",
    "        # Rotary position embedding\n",
    "        sinusoidal_pos = self.rotary_emb(seq_len)\n",
    "        q_rot, k_rot = apply_rotary_pos_emb(q, k, sinusoidal_pos)\n",
    "\n",
    "        # Apply the custom scaled dot-product attention\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(q_rot, k_rot, v, mask)\n",
    "\n",
    "        # Transpose and reshape back to (batch_size, seq_len, d_model)\n",
    "        scaled_attention = scaled_attention.transpose(1, 2).contiguous()\n",
    "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Apply the final linear layer to combine the heads\n",
    "        attn_output = self.dense(concat_attention)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layernorm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerModelWithROPE(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, d_model, num_heads, d_ff, output_size, num_layers, dropout=0.1):\n",
    "        super(TransformerModelWithROPE, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayerWithROPE(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_size)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)  # (batch_size, seq_len, d_model)\n",
    "        x = x.mean(dim=1)  # (batch_size, d_model)\n",
    "        x = self.fc(self.dropout(x))  # (batch_size, output_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "6KuAipzfAeXv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, d_model, num_heads, d_ff, output_size, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_size)\n",
    "        x = self.positional_encoding(x)  # (batch_size, seq_len, d_model)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)  # (batch_size, seq_len, d_model)\n",
    "        x = x.mean(dim=1)  # (batch_size, d_model)\n",
    "        x = self.fc(self.dropout(x))  # (batch_size, output_size)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel submissive ever</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feel playful enough try new combination</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>find broken piece feeling nothing feeling noth...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feel ecstatic worry make love automatic adica ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ive feeling really jealous friend rafia im ash...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161608</th>\n",
       "      <td>feeling nervous</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161609</th>\n",
       "      <td>feel like punished believing austin</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161610</th>\n",
       "      <td>look back little paragraph ive written feel bi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161611</th>\n",
       "      <td>feel inconvenienced trimmer blade dull</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161612</th>\n",
       "      <td>feel rather surprised claimed swedish prosecut...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161613 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence sentiment  label\n",
       "0                                    feel submissive ever   sadness      4\n",
       "1                 feel playful enough try new combination       joy      2\n",
       "2       find broken piece feeling nothing feeling noth...     anger      0\n",
       "3       feel ecstatic worry make love automatic adica ...       joy      2\n",
       "4       ive feeling really jealous friend rafia im ash...     anger      0\n",
       "...                                                   ...       ...    ...\n",
       "161608                                    feeling nervous      fear      1\n",
       "161609                feel like punished believing austin   sadness      4\n",
       "161610  look back little paragraph ive written feel bi...     anger      0\n",
       "161611             feel inconvenienced trimmer blade dull   sadness      4\n",
       "161612  feel rather surprised claimed swedish prosecut...  surprise      5\n",
       "\n",
       "[161613 rows x 3 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling cold im alone</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feel like im th grade shy wouldnt say anything...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feel like navy dress dangerous</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feel jaded chaser although</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel petty vicious mean defensive angry</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53866</th>\n",
       "      <td>feeling especially tender tendency get weepy h...</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53867</th>\n",
       "      <td>pryers feel like listening perhaps punished ba...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53868</th>\n",
       "      <td>promise never react even grievously provoked l...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53869</th>\n",
       "      <td>feel submissive spoiled</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53870</th>\n",
       "      <td>go period feeling like one could love unremark...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53871 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment  label\n",
       "0                               im feeling cold im alone     anger      0\n",
       "1      feel like im th grade shy wouldnt say anything...      fear      1\n",
       "2                         feel like navy dress dangerous     anger      0\n",
       "3                             feel jaded chaser although   sadness      4\n",
       "4                feel petty vicious mean defensive angry     anger      0\n",
       "...                                                  ...       ...    ...\n",
       "53866  feeling especially tender tendency get weepy h...      love      3\n",
       "53867  pryers feel like listening perhaps punished ba...   sadness      4\n",
       "53868  promise never react even grievously provoked l...   sadness      4\n",
       "53869                            feel submissive spoiled   sadness      4\n",
       "53870  go period feeling like one could love unremark...   sadness      4\n",
       "\n",
       "[53871 rows x 3 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feeling lake popular weekend summer huge parki...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>couldnt stop feeling threatened card grandmoth...</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feel way try ignored ignored got interested es...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling bitchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>know little feel special bond</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53866</th>\n",
       "      <td>think bottom line b story pierce feel need acc...</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53867</th>\n",
       "      <td>straight man ejacalute like week sexual intere...</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53868</th>\n",
       "      <td>feel really rude stating fact would feel rude ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53869</th>\n",
       "      <td>im feeling especially triggered grumpy note ev...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53870</th>\n",
       "      <td>feel useless home</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53871 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment  label\n",
       "0      feeling lake popular weekend summer huge parki...       joy      2\n",
       "1      couldnt stop feeling threatened card grandmoth...      fear      1\n",
       "2      feel way try ignored ignored got interested es...   sadness      4\n",
       "3                                         feeling bitchy     anger      0\n",
       "4                          know little feel special bond       joy      2\n",
       "...                                                  ...       ...    ...\n",
       "53866  think bottom line b story pierce feel need acc...      love      3\n",
       "53867  straight man ejacalute like week sexual intere...      love      3\n",
       "53868  feel really rude stating fact would feel rude ...     anger      0\n",
       "53869  im feeling especially triggered grumpy note ev...     anger      0\n",
       "53870                                  feel useless home   sadness      4\n",
       "\n",
       "[53871 rows x 3 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "GPU device name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "jdZcu7ZKBVBZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# Create datasets with device\n",
    "train_dataset = TextDataset(train_df, tokenizer, device)\n",
    "val_dataset = TextDataset(validation_df, tokenizer, device)\n",
    "test_dataset = TextDataset(test_df, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pryUvgK7ETbY",
    "outputId": "b1a06135-0920-43a7-f8ea-795d841e0f8c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36410,   850, 33532,  1683], device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for input_ids, label in train_dataset:\n",
    "    print(input_ids)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "id": "yxaHUt79BV5c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "id": "wfFQKb4-BX1q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Transformer model\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embed_size = 256\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "output_size = len(train_df['label'].unique())\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "\n",
    "# Initialize model and move to device\n",
    "#model = TransformerModel(vocab_size, embed_size, d_model, num_heads, d_ff, output_size, num_layers, dropout)\n",
    "model = TransformerModelWithROPE(vocab_size, embed_size, d_model, num_heads, d_ff, output_size, num_layers, dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''for input_ids, labels in val_dataloader:\n",
    "    outputs = model(input_ids)\n",
    "    print(outputs.size())\n",
    "    print(outputs)\n",
    "    print()\n",
    "    print(torch.max(outputs,1))\n",
    "    print()\n",
    "    _, predicted = torch.max(outputs,1)\n",
    "    print(_)\n",
    "    print(predicted)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the accuracy function (using the first version from before)\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    total = labels.size(0)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop with model saving\n",
    "best_val_accuracy = 0.0  # Track the best validation accuracy\n",
    "best_model_path = 'best_model.pth'  # File path to save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "TCDm8oP1BaMl",
    "outputId": "b0540645-1ca9-40d4-df6a-35af6b69c03b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training step\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for input_ids, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        batch_acc, batch_correct, batch_total = calculate_accuracy(outputs, labels)\n",
    "        train_correct += batch_correct\n",
    "        train_total += batch_total\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Correct/Total: {train_correct}/{train_total}, Training Accuracy after Epoch {epoch+1}: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, labels in val_dataloader:\n",
    "            outputs = model(input_ids)\n",
    "            batch_acc, batch_correct, batch_total = calculate_accuracy(outputs, labels)\n",
    "            val_correct += batch_correct\n",
    "            val_total += batch_total\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Correct/Total: {val_correct}/{val_total}, Validation Accuracy after Epoch {epoch+1}: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save model if validation accuracy improves\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    print()  # Add a blank line for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TransformerModelWithROPE:\n\tMissing key(s) in state_dict: \"encoder_layers.0.rotary_emb.inv_freq\", \"encoder_layers.1.rotary_emb.inv_freq\", \"encoder_layers.2.rotary_emb.inv_freq\". \n\tUnexpected key(s) in state_dict: \"positional_encoding.pe\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[383]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Assuming the same model architecture\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model.eval()  \u001b[38;5;66;03m# Set to evaluation mode if using for inference\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\projectml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for TransformerModelWithROPE:\n\tMissing key(s) in state_dict: \"encoder_layers.0.rotary_emb.inv_freq\", \"encoder_layers.1.rotary_emb.inv_freq\", \"encoder_layers.2.rotary_emb.inv_freq\". \n\tUnexpected key(s) in state_dict: \"positional_encoding.pe\". "
     ]
    }
   ],
   "source": [
    "# Assuming the same model architecture\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()  # Set to evaluation mode if using for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct/Total: 49965/53871, Accuracy: 92.75%\n"
     ]
    }
   ],
   "source": [
    "test_correct, test_total = 0, 0\n",
    "\n",
    "for input_ids, labels in test_dataloader:\n",
    "    outputs = model(input_ids)\n",
    "    batch_acc, batch_correct, batch_total = calculate_accuracy(outputs, labels)\n",
    "    test_correct += batch_correct\n",
    "    test_total += batch_total\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Correct/Total: {test_correct}/{test_total}, Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_emotion(sentence, model, tokenizer, device):\n",
    "    # Preprocess the input sentence\n",
    "    sentence = sentence.lower()\n",
    "    encoding = tokenizer.encode(sentence)\n",
    "    input_ids = torch.tensor(encoding, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        accuracy, predicted = torch.max(outputs, 1)\n",
    "          \n",
    "    return label_dict[predicted.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " feel like stunned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: feel like stunned\n",
      "Predicted emotion: surprise\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "test_sentence = input()\n",
    "predicted_emotion = predict_emotion(test_sentence, model, tokenizer, device)\n",
    "print(f\"Sentence: {test_sentence}\")\n",
    "print(f\"Predicted emotion: {predicted_emotion}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 605165,
     "sourceId": 1085454,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (projectml)",
   "language": "python",
   "name": "projectml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
